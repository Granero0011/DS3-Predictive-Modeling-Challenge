{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Wrangling_Ideas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Granero0011/DS3-Predictive-Modeling-Challenge/blob/master/Data_Wrangling_Ideas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySkD3JhntUOl",
        "colab_type": "text"
      },
      "source": [
        "# Add your ideas for data wrangling in a cell below. Add code if you would like or just ideas for features/ways to clean the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi9NIF7UtxMx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Population around each well, higher pop-density would mean higher wear\n",
        "2.   Does elevation interact with condition of well\n",
        "3.   Airpollution? compare polution in the area to the longevity of the well \n",
        "4.   Does a certain installer have longer lasting wells? \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbFYOCm6KK_8",
        "colab_type": "text"
      },
      "source": [
        "let's get a majority <-?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1S2sEZCuRSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###!!! DONT JUST COPY AND PASTE\n",
        "\n",
        "def MrClean(df):\n",
        "    df_t= df\n",
        "    df_t['gps_height'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['population'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['amount_tsh'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['gps_height'].fillna(df_t.groupby(['region', 'district_code'])['gps_height'].transform('mean'), inplace=True)\n",
        "    df_t['gps_height'].fillna(df_t.groupby(['region'])['gps_height'].transform('mean'), inplace=True)\n",
        "    df_t['gps_height'].fillna(df_t['gps_height'].mean(), inplace=True)\n",
        "    df_t['population'].fillna(df_t.groupby(['region', 'district_code'])['population'].transform('median'), inplace=True)\n",
        "    df_t['population'].fillna(df_t.groupby(['region'])['population'].transform('median'), inplace=True)\n",
        "    df_t['population'].fillna(df_t['population'].median(), inplace=True)\n",
        "    df_t['amount_tsh'].fillna(df_t.groupby(['region', 'district_code'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    df_t['amount_tsh'].fillna(df_t.groupby(['region'])['amount_tsh'].transform('median'), inplace=True)\n",
        "    df_t['amount_tsh'].fillna(df_t['amount_tsh'].median(), inplace=True)\n",
        "    features=['amount_tsh', 'gps_height', 'population']\n",
        "    scaler = MinMaxScaler(feature_range=(0,20))\n",
        "    df_t[features] = scaler.fit_transform(df_t[features])\n",
        "    df_t['longitude'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['latitude'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['construction_year'].replace(0.0, np.nan, inplace=True)\n",
        "    df_t['latitude'].fillna(df_t.groupby(['region', 'district_code'])['latitude'].transform('mean'), inplace=True)\n",
        "    df_t['longitude'].fillna(df_t.groupby(['region', 'district_code'])['longitude'].transform('mean'), inplace=True)\n",
        "    df_t['longitude'].fillna(df_t.groupby(['region'])['longitude'].transform('mean'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t.groupby(['region', 'district_code'])['construction_year'].transform('median'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t.groupby(['region'])['construction_year'].transform('median'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\n",
        "    df_t['construction_year'].fillna(df_t['construction_year'].median(), inplace=True)\n",
        "    df_t['date_recorded'] = pd.to_datetime(df_t['date_recorded'])\n",
        "    df_t['years_service'] = df_t.date_recorded.dt.year - df_t.construction_year\n",
        "   \n",
        "    garbage=['wpt_name','num_private','subvillage','region_code','recorded_by','management_group',\n",
        "         'extraction_type_group','extraction_type_class','scheme_name','payment',\n",
        "        'quality_group','quantity_group','source_type','source_class','waterpoint_type_group',\n",
        "        'ward','public_meeting','permit','date_recorded','construction_year']\n",
        "    df_t.drop(garbage,axis=1, inplace=True)\n",
        "    df_t.waterpoint_type = df_t.waterpoint_type.str.lower()\n",
        "    df_t.funder = df_t.funder.str.lower()\n",
        "    df_t.basin = df_t.basin.str.lower()\n",
        "    df_t.region = df_t.region.str.lower()\n",
        "    df_t.source = df_t.source.str.lower()\n",
        "    df_t.lga = df_t.lga.str.lower()\n",
        "    df_t.management = df_t.management.str.lower()\n",
        "    df_t.quantity = df_t.quantity.str.lower()\n",
        "    df_t.water_quality = df_t.water_quality.str.lower()\n",
        "    df_t.payment_type=df_t.payment_type.str.lower()\n",
        "    df_t.extraction_type=df_t.extraction_type.str.lower()\n",
        "    df_t[\"funder\"].fillna(\"other\", inplace=True)\n",
        "    df_t[\"scheme_management\"].fillna(\"other\", inplace=True)\n",
        "    df_t[\"installer\"].fillna(\"other\", inplace=True)\n",
        "    \n",
        "    #further spacial/location information\n",
        "    #https://www.kaggle.com/c/sf-crime/discussion/18853\n",
        "    \n",
        "    return df_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ekXkwsL8To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def con_matrix_analysis(model, feature_matrix, target_vector):\n",
        "  \n",
        "  x = model.predict(feature_matrix)\n",
        "  y = target_vector\n",
        "  \n",
        "  print(classification_report(y, x,\n",
        "        target_names=['Functional', 'Needs Repair', 'Not-Functional']))\n",
        "\n",
        "  con_matrix = pd.DataFrame(confusion_matrix(y, x), \n",
        "             columns=['Predicted Functional', 'Predicted Needs Repair', 'Predicted Not-Functional'], \n",
        "             index=['Actual Functional', 'Actual Needs Repair', 'Actual Not-Functional'])\n",
        "                            \n",
        "  sns.heatmap(data=con_matrix, cmap='cool')\n",
        "  plt.show();\n",
        "  return con_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1S61wznQ_TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_cardinality_check(n, df):\n",
        "  \"\"\"\n",
        "  Given a cardinality limit (n) and a dataframe this function will search the\n",
        "  dataframe for features above the cardinality limit, then create a dict\n",
        "  from the results\n",
        "  \"\"\"\n",
        "  \n",
        "  feature_list = []\n",
        "  \n",
        "  cardinality_value = []\n",
        "  \n",
        "  for _ in range(len(df.columns)):\n",
        "    if len(df[df.columns[_]].value_counts()) > n:\n",
        "      \n",
        "      feature_list.append(df.columns[_])\n",
        "      \n",
        "      cardinality_value.append(len(df[df.columns[_]].value_counts()))\n",
        "                               \n",
        "        \n",
        "  feature_dict = dict(zip(feature_list, cardinality_value))\n",
        "  \n",
        "  return feature_dict"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}